{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIO_INSP-TD3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mDvRpiV_VL63",
        "Up3F55PLeV3J",
        "PFfZb_HdfQ7H",
        "S-HyqgxtfUNQ",
        "LJGsG7OXhiz6",
        "v1fPG0Zlp4q6"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### install and basic setup"
      ],
      "metadata": {
        "id": "mDvRpiV_VL63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "make sure everything works in colab"
      ],
      "metadata": {
        "id": "Ckl426FmaJiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "%pip install -U tf-agents pyvirtualdisplay\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[box2d,atari,accept-rom-license]"
      ],
      "metadata": {
        "id": "q7-c_XgGVZll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "imports"
      ],
      "metadata": {
        "id": "MyejQfZmaObk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEOpwMk5UFt6"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "                   \n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "import gym\n",
        "\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "import copy\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "mpl.rc('animation', html='jshtml')"
      ],
      "metadata": {
        "id": "klDBBd0dcNF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('BipedalWalker-v3')\n",
        "obs = env.reset(seed=0)\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "env.render()"
      ],
      "metadata": {
        "id": "dWy5_Oc72JU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### functions"
      ],
      "metadata": {
        "id": "Up3F55PLeV3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### actor & critic"
      ],
      "metadata": {
        "id": "PFfZb_HdfQ7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim, max_actions):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.max_actions = max_actions\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.l3 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.l1(state))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = self.max_actions * torch.tanh(self.l3(x))\n",
        "        return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        # TD3 - https://arxiv.org/pdf/1802.09477.pdf\n",
        "\n",
        "        # Q1\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.l3 = nn.Linear(hidden_dim, 1) \n",
        "        \n",
        "        # Q2\n",
        "        self.l4 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.l5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.l6 = nn.Linear(hidden_dim, 1) \n",
        "\n",
        "    def forward(self, state, action): \n",
        "        sa = torch.cat([state, action], 1) \n",
        "\n",
        "        # compute Q1\n",
        "        c1 = F.relu(self.l1(sa))\n",
        "        c1 = F.relu(self.l2(c1))\n",
        "        q1 = self.l3(c1)\n",
        "\n",
        "        # compute Q2\n",
        "        c2 = F.relu(self.l4(sa))\n",
        "        c2 = F.relu(self.l5(c2))\n",
        "        q2 = self.l6(c2)\n",
        "\n",
        "        return (q1, q2)"
      ],
      "metadata": {
        "id": "DTkWq0toeXhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### experience replay"
      ],
      "metadata": {
        "id": "S-HyqgxtfUNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.stata import StataValueLabel\n",
        "class ExperienceReplay:\n",
        "  def __init__(self, state_dim, action_dim, batch_size, device, max_size=1e6):\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0  # pointer\n",
        "    self.size = 0\n",
        "\n",
        "    self.state = np.zeros((max_size, state_dim))\n",
        "    self.action = np.zeros((max_size, action_dim))\n",
        "    self.reward = np.zeros((max_size, 1))\n",
        "    self.next_state = np.zeros((max_size, state_dim))\n",
        "    self.dead = np.zeros((max_size, 1))\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    self.device = device\n",
        "\n",
        "  def store_transition(self, state, action, reward, new_state, dead):\n",
        "    self.state[self.ptr] = state\n",
        "    self.action[self.ptr] = action\n",
        "    self.reward[self.ptr] = reward\n",
        "    self.next_state[self.ptr] = new_state\n",
        "    self.dead[self.ptr] = dead\n",
        "\n",
        "    self.ptr = (self.ptr + 1) % self.max_size\n",
        "    self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "  def sample(self):\n",
        "    idx = np.random.randint(0, self.size, size=self.batch_size)\n",
        "\n",
        "    return (\n",
        "\t\t\ttorch.FloatTensor(self.state[idx]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[idx]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[idx]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[idx]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.dead[idx]).to(self.device)\n",
        "\t\t)\n",
        "\n",
        "  \n",
        "  def save(self):\n",
        "    # state, action, reward, next_state, dead, ptr, size\n",
        "    state_df = pd.DataFrame(self.state)\n",
        "    state_df.to_csv('state.csv', index=False)\n",
        "    action_df = pd.DataFrame(self.action)\n",
        "    action_df.to_csv('action.csv', index=False)\n",
        "    reward_df = pd.DataFrame(self.reward)\n",
        "    reward_df.to_csv('reward.csv', index=False)\n",
        "    next_state_df = pd.DataFrame(self.next_state)\n",
        "    next_state_df.to_csv('next_state.csv', index=False)\n",
        "    dead_df = pd.DataFrame(self.dead)\n",
        "    dead_df.to_csv('dead.csv', index=False)\n",
        "    ptr_df = pd.DataFrame([self.ptr], dtype=int)\n",
        "    ptr_df.to_csv('ptr.csv', index=False)\n",
        "    size_df = pd.DataFrame([self.size], dtype=int)\n",
        "    size_df.to_csv('size.csv', index=False)\n",
        "    print('experience replay saved')\n",
        "\n",
        "  def load(self):\n",
        "    self.state = pd.read_csv('state.csv').to_numpy()\n",
        "    self.action = pd.read_csv('action.csv').to_numpy()\n",
        "    self.reward = pd.read_csv('reward.csv').to_numpy()\n",
        "    self.next_state = pd.read_csv('next_state.csv').to_numpy()\n",
        "    self.dead = pd.read_csv('dead.csv').to_numpy()\n",
        "    self.ptr = pd.read_csv('ptr.csv').to_numpy()[0]\n",
        "    self.size = pd.read_csv('size.csv').to_numpy()[0]"
      ],
      "metadata": {
        "id": "jVA-XxRUfW0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### TD3"
      ],
      "metadata": {
        "id": "LJGsG7OXhiz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3():\n",
        "    def __init__(self, state_dim, action_dim, max_action, env, device, config):\n",
        "        super(TD3, self).__init__()\n",
        "\n",
        "        # set up actor\n",
        "        self.actor = Actor(state_dim, action_dim, config['hidden_dim'], max_action).to(device)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=config['learning_rate'])\n",
        "        self.device = device\n",
        "\n",
        "        # set up critic\n",
        "        self.critic = Critic(state_dim, action_dim, config['hidden_dim']).to(device)\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=config['learning_rate'])\n",
        "        self.max_action = max_action\n",
        "        self.env = env\n",
        "\n",
        "    def select_action(self, state, noise=0.1): \n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
        "        action = self.actor(state).cpu().data.numpy().flatten()\n",
        "        # add random amount of noise from a normal distribution to the action\n",
        "        if(noise == config['explore_policy']): \n",
        "            action = (action + np.random.normal(0, noise, size=self.env.action_space.shape[0]))\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.actor.state_dict(), 'actor.pth')\n",
        "        torch.save(self.actor.state_dict(), 'actor_target.pth')\n",
        "        torch.save(self.actor_optimizer.state_dict(), 'actor_opt.pth')\n",
        "        torch.save(self.critic.state_dict(), 'critic.pth')\n",
        "        torch.save(self.critic.state_dict(), 'critic_target.pth')\n",
        "        torch.save(self.critic_optimizer.state_dict(), 'critic_opt.pth')\n",
        "        return\n",
        "    \n",
        "    def load(self):\n",
        "        self.actor.load_state_dict(torch.load('actor.pth', map_location='cpu'))\n",
        "        self.actor_target.load_state_dict(torch.load('actor_target.pth', map_location='cpu'))\n",
        "        self.actor_optimizer.load_state_dict(torch.load('actor_opt.pth', map_location='cpu'))\n",
        "        self.critic.load_state_dict(torch.load('critic.pth', map_location='cpu'))\n",
        "        self.critic_target.load_state_dict(torch.load('critic_target.pth', map_location='cpu'))\n",
        "        self.critic_optimizer.load_state_dict(torch.load('critic_opt.pth', map_location='cpu'))\n",
        "        return\n",
        "\n",
        "    def train(self, replay_buffer, current_iteration): \n",
        "        # pseudocode: http://bicmr.pku.edu.cn/~wenzw/bigdata/lect-dyna3w.pdf\n",
        "\n",
        "        # sample batch transitions from replay_buffer.\n",
        "        state, action, reward, next_state, done = replay_buffer.sample()\n",
        "        tensor_cpy = action.clone().detach()\n",
        "        # sample noise and clip it\n",
        "        noise = tensor_cpy.normal_(0, config['noise_policy']).clamp(-config['noise_clip'], config['noise_clip'])\n",
        "\n",
        "        # clip (next action + clipped noise)\n",
        "        next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n",
        "        \n",
        "        # compute q1, q2, take min\n",
        "        target_q1, target_q2 = self.critic_target(next_state, next_action)\n",
        "        target_q = ((torch.min(target_q1, target_q2)) * (1-done)) + reward\n",
        "        curr_q1, curr_q2 = self.critic(state, action)\n",
        "\n",
        "        critic_loss = F.mse_loss(curr_q1, target_q) + F.mse_loss(curr_q2, target_q)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()  # update Q-functions\n",
        "\n",
        "        # learn policy by maximizing the current Q every other iteration\n",
        "        if (current_iteration % config['policy_delay'] == 0):\n",
        "            actor_loss = -self.critic(state, self.actor(state))[0].mean()\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            # if i % policy_delay == 0, then we update model (delayed updates)\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(config['tau'] * param.data + (1 - config['tau']) * target_param.data)\n",
        "\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):    \n",
        "                target_param.data.copy_(config['tau'] * param.data + (1 - config['tau']) * target_param.data)"
      ],
      "metadata": {
        "id": "zjRYr4zpk5or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### main"
      ],
      "metadata": {
        "id": "v1fPG0Zlp4q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config):\n",
        "    env = gym.make('BipedalWalker-v3')\n",
        "\n",
        "    # set seed for reproducable results\n",
        "    seed = 0\n",
        "    env.reset(seed=seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f'running on {device}')\n",
        "    \n",
        "\n",
        "    policy = TD3(state_dim, action_dim, max_action, env, device, config)\n",
        "    buffer = ExperienceReplay(state_dim, action_dim, config['batch_size'], device, config['buffer_size'])\n",
        "\n",
        "    if config['train_from_scratch']:\n",
        "      best_reward = -1*sys.maxsize\n",
        "      print('actor and critic training from scratch')\n",
        "      scores_over_episodes = []\n",
        "    else:\n",
        "      policy.load()\n",
        "      scores_over_episodes = [[float(y) for y in x] for x in pd.read_csv('TD3_res.csv').values]\n",
        "      best_reward = max([max(_) for _ in scores_over_episodes])\n",
        "      print(f'actor and critic loaded with max score: {round(best_reward, 3)}')\n",
        "      # buffer.load()\n",
        "      # print('experience replay loaded')\n",
        "\n",
        "    for episode in range(config['episodes']):\n",
        "        reward_list_cur_episode = []\n",
        "\n",
        "        # now do the actual training step\n",
        "        state = env.reset()\n",
        "        avg_reward = 0\n",
        "        for i in range(config['timesteps']):\n",
        "          # Same as the TD3, select an action and add noise:\n",
        "          action = policy.select_action(state) + np.random.normal(0, max_action * config['noise'], size=action_dim)\n",
        "          action = action.clip(env.action_space.low, env.action_space.high)\n",
        "          # Make an action. \n",
        "          if np.isnan(action).any():\n",
        "            continue\n",
        "          next_state, reward, done, _ = env.step(action)\n",
        "          buffer.store_transition(state, action, reward, next_state, done)\n",
        "          state = next_state\n",
        "          avg_reward += reward\n",
        "          env.render()\n",
        "          if(buffer.size > config['batch_size']):\n",
        "            policy.train(buffer, i)\n",
        "          if(done or i > config['timesteps']):\n",
        "            reward_list_cur_episode.append(avg_reward)\n",
        "            break\n",
        "\n",
        "        for _ in range(9):\n",
        "          state = env.reset()\n",
        "          avg_reward = 0\n",
        "          for i in range(config['timesteps']):\n",
        "            action = policy.select_action(state) + np.random.normal(0, max_action * config['noise'], size=action_dim)\n",
        "            action = action.clip(env.action_space.low, env.action_space.high)\n",
        "            if np.isnan(action).any():\n",
        "              continue\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            state = next_state\n",
        "            avg_reward += reward\n",
        "            if(done or i > config['timesteps']):\n",
        "              reward_list_cur_episode.append(avg_reward)\n",
        "              break\n",
        "\n",
        "        scores_over_episodes.append(reward_list_cur_episode)\n",
        "\n",
        "        print(f'{episode+1}/{config[\"episodes\"]} episodes finished')\n",
        "        print(f'{\" \"*4+\"min reward:\":<18}{round(min(scores_over_episodes[-1]), 3)}')\n",
        "        print(f'{\" \"*4+\"avg reward:\":<18}{round(sum(scores_over_episodes[-1])/len(scores_over_episodes[-1]), 3)}')\n",
        "        print(f'{\" \"*4+\"max reward:\":<18}{round(max(scores_over_episodes[-1]), 3)}')\n",
        "        print(f'{\" \"*4+\"timestap:\":<18}{i}')\n",
        "\n",
        "        if(episode >= 0 and max(scores_over_episodes[-1]) > best_reward):\n",
        "          print(' '*4+'saving agent - score was better than best-known score')\n",
        "          best_reward = max(scores_over_episodes[-1])\n",
        "          policy.save()\n",
        "\n",
        "    print('saving experience replay')\n",
        "    buffer.save()\n",
        "    \n",
        "    '''fig = plt.figure()\n",
        "    plt.plot(np.arange(1, len(scores_over_episodes) + 1), scores_over_episodes)\n",
        "    plt.ylabel('Score')\n",
        "    plt.xlabel('Episode #')\n",
        "    plt.show()'''\n",
        "\n",
        "    df = pd.DataFrame(scores_over_episodes)\n",
        "    df.to_csv('TD3_res.csv', index=False)\n",
        "    print('reward history saved')"
      ],
      "metadata": {
        "id": "wAF93mWMp31i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### run stuff"
      ],
      "metadata": {
        "id": "HjxWjNCs0m_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'batch_size' : 100, \n",
        "    'discount_factor' : 0.99, \n",
        "    'explore_policy' : 0.1, \n",
        "    'learning_rate' : 0.001, \n",
        "    'policy_delay' : 2, \n",
        "    'tau' : 0.005, \n",
        "    'noise_policy' : 0.2, \n",
        "    'noise_clip' : 0.5,\n",
        "    'save_score' : 400,\n",
        "    'episodes' : 100,  # 800\n",
        "    'timesteps' : 2000,\n",
        "    'buffer_size' : 1000000,\n",
        "    'noise' : 0.1,\n",
        "    'hidden_dim': 512, \n",
        "    'train_from_scratch': False\n",
        "}"
      ],
      "metadata": {
        "id": "wCtqLZa-fOQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(config)"
      ],
      "metadata": {
        "id": "1AlFu2NOncq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "2CIilEJMifxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "save files"
      ],
      "metadata": {
        "id": "QYOkPWjvJneb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"TD3_res.csv\") \n",
        "files.download(\"actor.pth\") \n",
        "files.download(\"actor_target.pth\") \n",
        "files.download(\"actor_opt.pth\") \n",
        "files.download(\"critic.pth\") \n",
        "files.download(\"critic_target.pth\") \n",
        "files.download(\"critic_opt.pth\") \n",
        "\n",
        "'''files.download(\"action.csv\") \n",
        "files.download(\"dead.csv\") \n",
        "files.download(\"next_state.csv\") \n",
        "files.download(\"ptr.csv\") \n",
        "files.download(\"reward.csv\") \n",
        "files.download(\"size.csv\") \n",
        "files.download(\"state.csv\")'''"
      ],
      "metadata": {
        "id": "F7btV_A1c1Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "iByk7ahHa3qQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make video"
      ],
      "metadata": {
        "id": "KDaqNrben97t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('BipedalWalker-v3')\n",
        "env = gym.wrappers.RecordVideo(env, 'video')\n",
        "\n",
        "# set seed for reproducable results\n",
        "seed = 0\n",
        "env.reset(seed=seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'running on {device}')\n",
        "policy = TD3(state_dim, action_dim, max_action, env, device, config)\n",
        "buffer = ExperienceReplay(state_dim, action_dim, config['batch_size'], device, config['buffer_size'])\n",
        "\n",
        "policy.load()\n",
        "scores_over_episodes = [[float(y) for y in x] for x in pd.read_csv('TD3_res.csv').values]\n",
        "best_reward = max([max(_) for _ in scores_over_episodes])\n",
        "print(f'actor and critic loaded with max score: {round(best_reward, 3)}')\n",
        "# buffer.load()\n",
        "# print('experience replay loaded')\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "avg_reward = 0\n",
        "while not done:\n",
        "  action = policy.select_action(state) + np.random.normal(0, max_action * config['noise'], size=action_dim)\n",
        "  action = action.clip(env.action_space.low, env.action_space.high)\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  state = next_state\n",
        "  avg_reward += reward\n",
        "print(f'avg_reward : {avg_reward}')\n",
        "env.close()"
      ],
      "metadata": {
        "id": "aOO4EYRQRpHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ReVvdsWlmvN1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}